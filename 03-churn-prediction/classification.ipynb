{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ac5e25-f615-4f68-8f98-7f6fb60442f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc2532b-03da-4813-8454-48764d281366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee54ae0d-e9bc-4250-826a-be79d2ba008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', 'employment_status', 'location', 'interaction_count', 'lead_score', 'converted']\n",
      "Missing values:\n",
      " lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "print(\"Missing values:\\n\", df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a9408b-111f-4499-a9d3-6b45eea80b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Numerical columns: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score', 'converted']\n",
      "\n",
      "Missing values after preprocessing:\n",
      " lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values for numerical columns\n",
    "#for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "#    df[col] = df[col].fillna(0)  # Fill with 0, as in Question 6\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "print(\"\\nCategorical columns:\", categorical_cols.tolist())\n",
    "print(\"Numerical columns:\", numerical_cols.tolist())\n",
    "\n",
    "# Replace missing values\n",
    "# Categorical: fill with 'NA'\n",
    "#for col in categorical_cols:\n",
    "#    df[col] = df[col].fillna('NA')\n",
    "\n",
    "# Numerical: fill with 0.0\n",
    "#for col in numerical_cols:\n",
    "#    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "\n",
    "# Data preparation: Fill missing values\n",
    "# Categorical: fill with 'NA'\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "\n",
    "# Numerical: fill with 0.0\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "# Verify missing values after preprocessing\n",
    "print(\"\\nMissing values after preprocessing:\\n\", df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3906d3cb-2889-46fa-861e-ad12eb255985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NA</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>referral</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>north_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>referral</td>\n",
       "      <td>technology</td>\n",
       "      <td>3</td>\n",
       "      <td>65259.0</td>\n",
       "      <td>student</td>\n",
       "      <td>europe</td>\n",
       "      <td>2</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>technology</td>\n",
       "      <td>1</td>\n",
       "      <td>45688.0</td>\n",
       "      <td>student</td>\n",
       "      <td>north_america</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>referral</td>\n",
       "      <td>NA</td>\n",
       "      <td>5</td>\n",
       "      <td>71016.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>north_america</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>organic_search</td>\n",
       "      <td>finance</td>\n",
       "      <td>3</td>\n",
       "      <td>92855.0</td>\n",
       "      <td>student</td>\n",
       "      <td>north_america</td>\n",
       "      <td>3</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1462 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lead_source       industry  number_of_courses_viewed  annual_income  \\\n",
       "0           paid_ads             NA                         1        79450.0   \n",
       "1       social_media         retail                         1        46992.0   \n",
       "2             events     healthcare                         5        78796.0   \n",
       "3           paid_ads         retail                         2        83843.0   \n",
       "4           referral      education                         3        85012.0   \n",
       "...              ...            ...                       ...            ...   \n",
       "1457        referral  manufacturing                         1            0.0   \n",
       "1458        referral     technology                         3        65259.0   \n",
       "1459        paid_ads     technology                         1        45688.0   \n",
       "1460        referral             NA                         5        71016.0   \n",
       "1461  organic_search        finance                         3        92855.0   \n",
       "\n",
       "     employment_status       location  interaction_count  lead_score  \\\n",
       "0           unemployed  south_america                  4        0.94   \n",
       "1             employed  south_america                  1        0.80   \n",
       "2           unemployed      australia                  3        0.69   \n",
       "3                   NA      australia                  1        0.87   \n",
       "4        self_employed         europe                  3        0.62   \n",
       "...                ...            ...                ...         ...   \n",
       "1457     self_employed  north_america                  4        0.53   \n",
       "1458           student         europe                  2        0.24   \n",
       "1459           student  north_america                  3        0.02   \n",
       "1460     self_employed  north_america                  0        0.25   \n",
       "1461           student  north_america                  3        0.41   \n",
       "\n",
       "      converted  \n",
       "0             1  \n",
       "1             0  \n",
       "2             1  \n",
       "3             0  \n",
       "4             1  \n",
       "...         ...  \n",
       "1457          1  \n",
       "1458          1  \n",
       "1459          1  \n",
       "1460          1  \n",
       "1461          1  \n",
       "\n",
       "[1462 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6201a7f0-2897-42de-935f-7ff5cc6f3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', 'employment_status', 'location', 'interaction_count', 'lead_score', 'converted']\n",
      "\n",
      "Missing values in 'industry': 0\n",
      "\n",
      "Value counts for 'industry':\n",
      " industry\n",
      "retail           203\n",
      "finance          200\n",
      "other            198\n",
      "healthcare       187\n",
      "education        187\n",
      "technology       179\n",
      "manufacturing    174\n",
      "NA               134\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "print(\"\\nMissing values in 'industry':\", df['industry'].isna().sum())\n",
    "print(\"\\nValue counts for 'industry':\\n\", df['industry'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192303a1-81b9-4251-82d6-7792b78cc071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode of 'industry' (raw data): retail\n"
     ]
    }
   ],
   "source": [
    "# Compute the mode of the 'industry' column (raw data)\n",
    "industry_mode = df['industry'].mode()[0]\n",
    "print(\"\\nMode of 'industry' (raw data):\", industry_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0003573e-d618-4f9a-b996-34468e74f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode of 'industry' (after filling with 'NA'): retail\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in 'industry' with 'NA' (per data preparation)\n",
    "df['industry_filled'] = df['industry'].fillna('NA')\n",
    "\n",
    "# Compute the mode after filling missing values\n",
    "industry_mode_filled = df['industry_filled'].mode()[0]\n",
    "print(\"Mode of 'industry' (after filling with 'NA'):\", industry_mode_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c01b8f-eaed-440f-a143-c593382005a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between interaction_count and lead_score: 0.010 (Absolute: 0.010)\n",
      "Correlation between number_of_courses_viewed and lead_score: -0.005 (Absolute: 0.005)\n",
      "Correlation between number_of_courses_viewed and interaction_count: -0.024 (Absolute: 0.024)\n",
      "Correlation between annual_income and interaction_count: 0.027 (Absolute: 0.027)\n"
     ]
    }
   ],
   "source": [
    "# Define the pairs to check\n",
    "pairs = [\n",
    "    ('interaction_count', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'interaction_count'),\n",
    "    ('annual_income', 'interaction_count')\n",
    "]\n",
    "# Compute Pearson correlations\n",
    "correlations = {}\n",
    "for col1, col2 in pairs:\n",
    "    corr = df[col1].corr(df[col2])\n",
    "    abs_corr = abs(corr)\n",
    "    correlations[f\"{col1} and {col2}\"] = (corr, abs_corr)\n",
    "    print(f\"Correlation between {col1} and {col2}: {corr:.3f} (Absolute: {abs_corr:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7419a34a-1104-442f-95ea-8343130d0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair with highest absolute correlation: annual_income and interaction_count\n",
      "Absolute correlation value: 0.027\n",
      "Answer: annual_income and interaction_count\n"
     ]
    }
   ],
   "source": [
    "# Find the pair with the highest absolute correlation\n",
    "max_pair = max(correlations, key=lambda x: correlations[x][1])\n",
    "max_abs_corr = correlations[max_pair][1]\n",
    "\n",
    "print(f\"\\nPair with highest absolute correlation: {max_pair}\")\n",
    "print(f\"Absolute correlation value: {max_abs_corr:.3f}\")\n",
    "print(f\"Answer: {max_pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f793ead7-2ee7-4796-814a-f9e25490993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required columns\n",
    "required_cols = ['industry', 'location', 'lead_source', 'employment_status']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: Missing columns {missing_cols}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f33143f-1481-4e61-be78-0f42aea28c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume target is 'converted' (binary classification)\n",
    "target = 'converted'\n",
    "if target not in df.columns:\n",
    "    print(f\"Error: Target '{target}' not found. Available columns:\", df.columns.tolist())\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6436365-97b4-4b59-b8cf-4040d560d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features for MI calculation\n",
    "X = df[required_cols].copy()\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c9e995-7e7a-4cd6-b1c9-6eb1dfaf2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "label_encoders = {}\n",
    "for col in required_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a0fa81-7ac0-4198-a40b-d3fa1ee013ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mutual information scores\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "mi_dict = {col: score for col, score in zip(required_cols, mi_scores)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a753d4-422e-4443-b5ec-9416cfbed2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI score for industry: 0.015\n",
      "MI score for location: 0.000\n",
      "MI score for lead_source: 0.037\n",
      "MI score for employment_status: 0.033\n",
      "\n",
      "Feature with highest MI: lead_source\n",
      "Highest MI score: 0.037\n",
      "Answer: lead_source\n"
     ]
    }
   ],
   "source": [
    "# Print MI scores\n",
    "for col, score in mi_dict.items():\n",
    "    print(f\"MI score for {col}: {score:.3f}\")\n",
    "\n",
    "# Find the feature with the highest MI score\n",
    "max_feature = max(mi_dict, key=mi_dict.get)\n",
    "max_mi = mi_dict[max_feature]\n",
    "\n",
    "print(f\"\\nFeature with highest MI: {max_feature}\")\n",
    "print(f\"Highest MI score: {max_mi:.3f}\")\n",
    "print(f\"Answer: {max_feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb53091-d3ee-4c87-8a10-3167989a6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec37bd9e-70c2-4577-b767-7d60b4c166d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume target is 'converted' (binary classification)\n",
    "target = 'converted'\n",
    "if target not in df_encoded.columns:\n",
    "    print(f\"Error: Target '{target}' not found. Available columns:\", df_encoded.columns.tolist())\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54b5cc14-f4d7-47ae-97e6-e24b22b6af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_encoded.drop(target, axis=1)\n",
    "y = df_encoded[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc3c2a2b-9100-4634-8c11-f23c7b46d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 60% train, 20% validation, 20% test with seed 42\n",
    "X_full, X_test, y_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_full, y_full, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10bb5b76-5cf5-46cd-a122-e7bda900d6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train dtypes:\n",
      " number_of_courses_viewed             int64\n",
      "annual_income                      float64\n",
      "interaction_count                    int64\n",
      "lead_score                         float64\n",
      "industry_filled                     object\n",
      "lead_source_events                    bool\n",
      "lead_source_organic_search            bool\n",
      "lead_source_paid_ads                  bool\n",
      "lead_source_referral                  bool\n",
      "lead_source_social_media              bool\n",
      "industry_education                    bool\n",
      "industry_finance                      bool\n",
      "industry_healthcare                   bool\n",
      "industry_manufacturing                bool\n",
      "industry_other                        bool\n",
      "industry_retail                       bool\n",
      "industry_technology                   bool\n",
      "employment_status_employed            bool\n",
      "employment_status_self_employed       bool\n",
      "employment_status_student             bool\n",
      "employment_status_unemployed          bool\n",
      "location_africa                       bool\n",
      "location_asia                         bool\n",
      "location_australia                    bool\n",
      "location_europe                       bool\n",
      "location_middle_east                  bool\n",
      "location_north_america                bool\n",
      "location_south_america                bool\n",
      "dtype: object\n",
      "X_train missing values:\n",
      " number_of_courses_viewed           0\n",
      "annual_income                      0\n",
      "interaction_count                  0\n",
      "lead_score                         0\n",
      "industry_filled                    0\n",
      "lead_source_events                 0\n",
      "lead_source_organic_search         0\n",
      "lead_source_paid_ads               0\n",
      "lead_source_referral               0\n",
      "lead_source_social_media           0\n",
      "industry_education                 0\n",
      "industry_finance                   0\n",
      "industry_healthcare                0\n",
      "industry_manufacturing             0\n",
      "industry_other                     0\n",
      "industry_retail                    0\n",
      "industry_technology                0\n",
      "employment_status_employed         0\n",
      "employment_status_self_employed    0\n",
      "employment_status_student          0\n",
      "employment_status_unemployed       0\n",
      "location_africa                    0\n",
      "location_asia                      0\n",
      "location_australia                 0\n",
      "location_europe                    0\n",
      "location_middle_east               0\n",
      "location_north_america             0\n",
      "location_south_america             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check shapes\n",
    "#print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Debug: Check for non-numerical columns and missing values\n",
    "print(f\"\\nX_train dtypes:\\n\", X_train.dtypes)\n",
    "print(f\"X_train missing values:\\n\", X_train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c47be1f1-6692-4d79-84f0-0f15731f0ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'retail'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Scale numerical features\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#X_train_processed = pd.get_dummies(X_train, columns=['customer_type'], drop_first=True)\u001b[39;00m\n\u001b[32m      3\u001b[39m scaler = StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m X_val_scaled = scaler.transform(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/sklearn/utils/validation.py:971\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    970\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    973\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/generic.py:6662\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6656\u001b[39m     results = [\n\u001b[32m   6657\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6658\u001b[39m     ]\n\u001b[32m   6660\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6661\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6662\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6663\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/internals/managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/internals/blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/machine-learning-zoomcamp-homework/mlzoomcamp/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'retail'"
     ]
    }
   ],
   "source": [
    "# Scale numerical features\n",
    "#X_train_processed = pd.get_dummies(X_train, columns=['customer_type'], drop_first=True)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124ede0-ab52-45a8-9dae-4c67ba66763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check shapes\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3a8cf-4fd9-4ef3-8d90-98e568010fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c6438-5184-4a82-84c1-54ea626d9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy on validation set\n",
    "baseline_accuracy = accuracy_score(y_val, y_pred)\n",
    "accuracy_rounded = round(baseline_accuracy, 2)\n",
    "print(f\"Accuracy on validation set: {accuracy_rounded}\")\n",
    "print(f\"Answer: {accuracy_rounded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6051b-807c-4d29-9625-1b48175db038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to eliminate\n",
    "features_to_test = ['industry', 'employment_status', 'lead_score']\n",
    "\n",
    "# Store accuracy differences\n",
    "accuracy_differences = {}\n",
    "\n",
    "# Feature elimination\n",
    "for feature in features_to_test:\n",
    "    # Identify columns to drop\n",
    "    if feature in ['industry', 'employment_status']:\n",
    "        # Drop all one-hot encoded columns for the categorical feature\n",
    "        drop_cols = [col for col in X.columns if col.startswith(f\"{feature}_\")]\n",
    "    else:\n",
    "        # Drop the single numerical column\n",
    "        drop_cols = [feature]\n",
    "    \n",
    "    if not drop_cols:\n",
    "        print(f\"Warning: No columns found for {feature}\")\n",
    "        continue\n",
    "    \n",
    "    # Create new feature set\n",
    "    X_train_temp = X_train.drop(columns=drop_cols)\n",
    "    X_val_temp = X_val.drop(columns=drop_cols)\n",
    "    \n",
    "    # Scale temporary features\n",
    "    scaler_temp = StandardScaler()\n",
    "    X_train_temp_scaled = scaler_temp.fit_transform(X_train_temp)\n",
    "    X_val_temp_scaled = scaler_temp.transform(X_val_temp)\n",
    "    \n",
    "    # Train model without the feature\n",
    "    model_temp = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_temp.fit(X_train_temp_scaled, y_train)\n",
    "    y_pred_temp = model_temp.predict(X_val_temp_scaled)\n",
    "    accuracy_temp = accuracy_score(y_val, y_pred_temp)\n",
    "    \n",
    "    # Calculate difference\n",
    "    difference = baseline_accuracy - accuracy_temp\n",
    "    accuracy_differences[feature] = difference\n",
    "    print(f\"Accuracy without {feature}: {accuracy_temp:.6f}, Difference: {difference:.6f}\")\n",
    "\n",
    "# Find feature with smallest absolute difference\n",
    "min_feature = min(accuracy_differences, key=lambda x: abs(accuracy_differences[x]))\n",
    "min_difference = accuracy_differences[min_feature]\n",
    "\n",
    "print(f\"\\nFeature with smallest absolute difference: {min_feature}\")\n",
    "print(f\"Smallest absolute difference: {abs(min_difference):.6f}\")\n",
    "print(f\"Answer: {min_feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0a166-1e36-4b4e-a148-00f7be5c37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define C values to test\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Store accuracies\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ea186-dc83-4509-bc4f-37c69618cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each C value\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    accuracy_rounded = round(accuracy, 3)\n",
    "    accuracies[C] = accuracy_rounded\n",
    "    print(f\"Accuracy for C={C}: {accuracy_rounded}\")\n",
    "\n",
    "# Find the best C (smallest C if tied)\n",
    "best_C = min((C for C, acc in accuracies.items() if acc == max(accuracies.values())), key=lambda x: x)\n",
    "best_accuracy = accuracies[best_C]\n",
    "\n",
    "print(f\"\\nBest C: {best_C}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "print(f\"Answer: {best_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03b9b1-2ad1-4b5b-be28-29b22f2efe65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
